## Lab 2

***
Lab scripts: 
* How to train a perceptron on MNIST with PyTorch: [perceptron_example_mnist.py](./perceptron_example_mnist.py)

***
For self-study (all students):
* From the previous lab:
  * [Essence of linear algebra](https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab) (linear transformations; matrix multiplication)
  * [Essence of calculus](https://www.youtube.com/playlist?list=PLZHQObOWTQDMsr9K-rj53DwVRMYO3t5Yr) (derivatives; chain rule)
  * [Neural Networks (chapter 1 - chapter 4)](https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi) (animated introduction to neural networks and backpropagation)
  * [Backpropagation example, from scratch](https://drive.google.com/file/d/11pFnI-NvGjAPgBb2nZKVj2dtE3dVI8qe/view)
* From this lab:
  * [CNN Explainer](https://poloclub.github.io/cnn-explainer/)
  * [But what is a convolution?](https://www.youtube.com/watch?v=KuXjwB4LzSA) (convolution example; convolutions in image processing; convolutions and polynomial multiplication; FFT)
  * https://paperswithcode.com/method/1x1-convolution

Advanced (for students who want to learn more):
* `pin_memory` & `non_blocking=True`:
   * https://pytorch.org/docs/stable/notes/cuda.html#use-pinned-memory-buffers
   * Pinning memory in DataLoaders: https://pytorch.org/docs/stable/notes/cuda.html#use-pinned-memory-buffers
   * How does pinned memory actually work: https://developer.nvidia.com/blog/how-optimize-data-transfers-cuda-cc/ 
   * Also see this discussion: https://discuss.pytorch.org/t/should-we-set-non-blocking-to-true/38234/4

***
References:
 - TODO