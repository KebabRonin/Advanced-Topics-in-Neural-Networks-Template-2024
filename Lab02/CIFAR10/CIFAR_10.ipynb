{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tensor-Reloaded/Advanced-Topics-in-Neural-Networks-Template-2024/blob/main/Lab02/CIFAR10/CIFAR_10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "nhbNpDWGnjG_",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nhbNpDWGnjG_",
        "outputId": "ccab909f-4e62-44ea-c3ab-78da8bc80e7b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting timed-decorator\n",
            "  Downloading timed_decorator-1.5.2-py3-none-any.whl.metadata (18 kB)\n",
            "Downloading timed_decorator-1.5.2-py3-none-any.whl (12 kB)\n",
            "Installing collected packages: timed-decorator\n",
            "Successfully installed timed-decorator-1.5.2\n"
          ]
        }
      ],
      "source": [
        "%pip install timed-decorator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "8H-nbkBWnlc6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8H-nbkBWnlc6",
        "outputId": "4f64779d-8a4c-4054-db3b-38e60441ef0b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting triton\n",
            "  Downloading triton-3.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from triton) (3.16.1)\n",
            "Downloading triton-3.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (209.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.4/209.4 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: triton\n",
            "Successfully installed triton-3.0.0\n"
          ]
        }
      ],
      "source": [
        "%pip install triton"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "-PAsUAbZnftE",
      "metadata": {
        "id": "-PAsUAbZnftE"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from multiprocessing import freeze_support\n",
        "from typing import Optional\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from timed_decorator.simple_timed import timed\n",
        "from torch import Tensor, nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torchvision.transforms import v2\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "oh1MgCN1ngwF",
      "metadata": {
        "id": "oh1MgCN1ngwF"
      },
      "outputs": [],
      "source": [
        "def get_default_device():\n",
        "    if torch.cuda.is_available():\n",
        "        return torch.device('cuda')\n",
        "    if torch.backends.mps.is_available():\n",
        "        return torch.device('mps')\n",
        "    return torch.device('cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "E0dqEsC4npOQ",
      "metadata": {
        "id": "E0dqEsC4npOQ"
      },
      "outputs": [],
      "source": [
        "class PreActBlock(nn.Module):\n",
        "    \"\"\"Pre-activation version of the BasicBlock.\"\"\"\n",
        "\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(PreActBlock, self).__init__()\n",
        "        self.bn1 = nn.BatchNorm2d(in_planes)\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False\n",
        "        )\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(\n",
        "            planes, planes, kernel_size=3, stride=1, padding=1, bias=False\n",
        "        )\n",
        "\n",
        "        if stride != 1 or in_planes != self.expansion * planes:\n",
        "            self.shortcut = nn.Conv2d(\n",
        "                in_planes,\n",
        "                self.expansion * planes,\n",
        "                kernel_size=1,\n",
        "                stride=stride,\n",
        "                bias=False,\n",
        "            )\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        out = F.relu(self.bn1(x), inplace=True)\n",
        "        shortcut = self.shortcut(out) if hasattr(self, \"shortcut\") else x\n",
        "        out = self.conv1(out)\n",
        "        out = self.conv2(F.relu(self.bn2(out), inplace=True))\n",
        "        out += shortcut\n",
        "        return out\n",
        "\n",
        "\n",
        "class PreActBottleneck(nn.Module):\n",
        "    \"\"\"Pre-activation version of the original Bottleneck module.\"\"\"\n",
        "\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(PreActBottleneck, self).__init__()\n",
        "        self.bn1 = nn.BatchNorm2d(in_planes)\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(\n",
        "            planes, planes, kernel_size=3, stride=stride, padding=1, bias=False\n",
        "        )\n",
        "        self.bn3 = nn.BatchNorm2d(planes)\n",
        "        self.conv3 = nn.Conv2d(\n",
        "            planes, self.expansion * planes, kernel_size=1, bias=False\n",
        "        )\n",
        "\n",
        "        if stride != 1 or in_planes != self.expansion * planes:\n",
        "            self.shortcut = nn.Conv2d(\n",
        "                in_planes,\n",
        "                self.expansion * planes,\n",
        "                kernel_size=1,\n",
        "                stride=stride,\n",
        "                bias=False,\n",
        "            )\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        out = F.relu(self.bn1(x), inplace=True)\n",
        "        shortcut = self.shortcut(out) if hasattr(self, \"shortcut\") else x\n",
        "        out = self.conv1(out)\n",
        "        out = self.conv2(F.relu(self.bn2(out), inplace=True))\n",
        "        out = self.conv3(F.relu(self.bn3(out), inplace=True))\n",
        "        out += shortcut\n",
        "        return out\n",
        "\n",
        "\n",
        "class PreActResNet_C10(nn.Module):\n",
        "    \"\"\"Pre-activation ResNet for CIFAR-10\"\"\"\n",
        "\n",
        "    def __init__(self, block, num_blocks, num_classes):\n",
        "        super(PreActResNet_C10, self).__init__()\n",
        "        self.in_planes = 64\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
        "        self.linear = nn.Linear(512 * block.expansion, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1] * (num_blocks - 1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        out = self.conv1(x)\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = F.avg_pool2d(out, 4)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "def PreActResNet18_C10(num_classes):\n",
        "    return PreActResNet_C10(PreActBlock, [2, 2, 2, 2], num_classes)\n",
        "\n",
        "\n",
        "def get_model():\n",
        "    return PreActResNet18_C10(10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "ga0_rh4sns61",
      "metadata": {
        "id": "ga0_rh4sns61"
      },
      "outputs": [],
      "source": [
        "class CachedDataset(Dataset):\n",
        "    def __init__(self, dataset: Dataset, runtime_transforms: Optional[v2.Transform], cache: bool):\n",
        "        if cache:\n",
        "            dataset = tuple([x for x in dataset])\n",
        "        self.dataset = dataset\n",
        "        self.runtime_transforms = runtime_transforms\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        image, label = self.dataset[i]\n",
        "        if self.runtime_transforms is None:\n",
        "            return image, label\n",
        "        return self.runtime_transforms(image), label\n",
        "\n",
        "def get_dataset(data_path: str, is_train: bool):\n",
        "    initial_transforms = v2.Compose([\n",
        "        v2.ToImage(),\n",
        "        v2.ToDtype(torch.float32, scale=True),\n",
        "        v2.Normalize(\n",
        "            mean=(0.491, 0.482, 0.446),\n",
        "            std=(0.247, 0.243, 0.261)\n",
        "        ),\n",
        "    ])\n",
        "    cifar10 = CIFAR10(root=data_path, train=is_train, transform=initial_transforms, download=True)\n",
        "    runtime_transforms = None\n",
        "    if is_train:\n",
        "        runtime_transforms = v2.Compose([\n",
        "            v2.RandomCrop(size=32, padding=4),\n",
        "            v2.RandomHorizontalFlip(),\n",
        "            v2.RandomVerticalFlip(),\n",
        "            v2.RandomErasing()\n",
        "        ])\n",
        "    return CachedDataset(cifar10, runtime_transforms, True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "XbmFnPgTnNC5",
      "metadata": {
        "id": "XbmFnPgTnNC5"
      },
      "outputs": [],
      "source": [
        "@torch.jit.script\n",
        "def accuracy(output: Tensor, labels: Tensor):\n",
        "    fp_plus_fn = torch.logical_not(output == labels).sum().item()\n",
        "    all_elements = len(output)\n",
        "    return (all_elements - fp_plus_fn) / all_elements\n",
        "\n",
        "\n",
        "def train(model, train_loader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "\n",
        "    all_outputs = []\n",
        "    all_labels = []\n",
        "\n",
        "    for data, labels in train_loader:\n",
        "        data = data.to(device, non_blocking=True)\n",
        "        labels = labels.to(device, non_blocking=True)\n",
        "        output = model(data)\n",
        "        loss = criterion(output, labels)\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        # torch.nn.utils.clip_grad_norm_(model.parameters(), 5)\n",
        "\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "        output = output.softmax(dim=1).detach().cpu().squeeze()\n",
        "        labels = labels.cpu().squeeze()\n",
        "        all_outputs.append(output)\n",
        "        all_labels.append(labels)\n",
        "\n",
        "    all_outputs = torch.cat(all_outputs).argmax(dim=1)\n",
        "    all_labels = torch.cat(all_labels)\n",
        "\n",
        "    return round(accuracy(all_outputs, all_labels), 4)\n",
        "\n",
        "\n",
        "@torch.inference_mode()\n",
        "def val(model, val_loader, device):\n",
        "    model.eval()\n",
        "\n",
        "    all_outputs = []\n",
        "    all_labels = []\n",
        "\n",
        "    for data, labels in val_loader:\n",
        "        data = data.to(device, non_blocking=True)\n",
        "        output = model(data)\n",
        "\n",
        "        output = output.softmax(dim=1).cpu().squeeze()\n",
        "        labels = labels.squeeze()\n",
        "        all_outputs.append(output)\n",
        "        all_labels.append(labels)\n",
        "\n",
        "    all_outputs = torch.cat(all_outputs).argmax(dim=1)\n",
        "    all_labels = torch.cat(all_labels)\n",
        "\n",
        "    return round(accuracy(all_outputs, all_labels), 4)\n",
        "\n",
        "\n",
        "def do_epoch(model, train_loader, val_loader, criterion, optimizer, device):\n",
        "    acc = train(model, train_loader, criterion, optimizer, device)\n",
        "    acc_val = val(model, val_loader, device)\n",
        "    # torch.cuda.empty_cache()\n",
        "    return acc, acc_val\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "Bd7kyNEJn2_F",
      "metadata": {
        "id": "Bd7kyNEJn2_F"
      },
      "outputs": [],
      "source": [
        "def main(device: torch.device = get_default_device(), data_path: str = './data',\n",
        "         checkpoint_path: str = \"./checkpoints\"):\n",
        "    print(f\"Using {device}\")\n",
        "    os.makedirs(checkpoint_path, exist_ok=True)\n",
        "    if device.type == 'cuda':\n",
        "        torch.backends.cudnn.benchmark = True\n",
        "        torch.backends.cuda.matmul.allow_tf32 = True\n",
        "\n",
        "    train_dataset = get_dataset(data_path, is_train=True)\n",
        "    val_dataset = get_dataset(data_path, is_train=False)\n",
        "\n",
        "    model = get_model()\n",
        "    model = model.to(device)\n",
        "    model = torch.jit.script(model)\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9, nesterov=True, weight_decay=0.00001,\n",
        "                                fused=True)\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.2, patience=10,\n",
        "                                                           threshold=0.001, threshold_mode='rel')\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "    batch_size = 50\n",
        "    val_batch_size = 500\n",
        "    num_workers = 0\n",
        "    persistent_workers = (num_workers != 0) and False\n",
        "    train_loader = DataLoader(train_dataset, shuffle=True, pin_memory=(device.type == 'cuda'), num_workers=num_workers,\n",
        "                              batch_size=batch_size, drop_last=True, persistent_workers=persistent_workers)\n",
        "    val_loader = DataLoader(val_dataset, shuffle=False, pin_memory=True, num_workers=0, batch_size=val_batch_size,\n",
        "                            drop_last=False)\n",
        "\n",
        "    epochs = tuple(range(200))\n",
        "    best_val = 0.0\n",
        "    with tqdm(epochs) as tbar:\n",
        "        for _ in tbar:\n",
        "            acc, acc_val = do_epoch(model, train_loader, val_loader, criterion, optimizer, device)\n",
        "            scheduler.step(acc)\n",
        "\n",
        "            if acc_val > best_val:\n",
        "                torch.save(model.state_dict(), os.path.join(checkpoint_path, \"best.pth\"))\n",
        "                best_val = acc_val\n",
        "            tbar.set_description(f\"Acc: {acc}, Acc_val: {acc_val}, Best_val: {best_val}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "RAhy_VyEndMH",
      "metadata": {
        "id": "RAhy_VyEndMH"
      },
      "outputs": [],
      "source": [
        "@timed(stdout=False, return_time=True)\n",
        "def infer(model, val_loader, device, tta, dtype, inference_mode):\n",
        "    model.eval()\n",
        "    all_outputs = []\n",
        "    all_labels = []\n",
        "\n",
        "    inference_mode = torch.inference_mode if inference_mode else torch.no_grad\n",
        "\n",
        "    enable_autocast = device.type != 'cpu' and dtype != torch.float32\n",
        "    # Autocast is slow for cpu, so we disable it.\n",
        "    # Also, if the device type is mps, autocast might not work (?) and disabling it might also not work (?)\n",
        "    with torch.autocast(device_type=device.type, dtype=dtype, enabled=enable_autocast), inference_mode():\n",
        "        for data, labels in val_loader:\n",
        "            data = data.to(device, non_blocking=True)\n",
        "\n",
        "            output = model(data)\n",
        "            if tta:\n",
        "                # Horizontal rotation:\n",
        "                output += model(v2.functional.hflip(data))\n",
        "                # Vertical rotation:\n",
        "                output += model(v2.functional.vflip(data))\n",
        "                # Horizontal rotation + Vertical rotation:\n",
        "                output += model(v2.functional.hflip(v2.functional.vflip(data)))\n",
        "\n",
        "            output = output.softmax(dim=1).cpu().squeeze()\n",
        "            labels = labels.squeeze()\n",
        "            all_outputs.append(output)\n",
        "            all_labels.append(labels)\n",
        "\n",
        "    all_outputs = torch.cat(all_outputs).argmax(dim=1)\n",
        "    all_labels = torch.cat(all_labels)\n",
        "\n",
        "    return round(accuracy(all_outputs, all_labels), 4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "BQt0O-lanWbZ",
      "metadata": {
        "id": "BQt0O-lanWbZ"
      },
      "outputs": [],
      "source": [
        "def create_model(device: torch.device, checkpoint_path: str, model_type: str):\n",
        "    model = get_model()\n",
        "    model = model.to(device)\n",
        "    model.load_state_dict(torch.load(os.path.join(checkpoint_path, \"best.pth\"), map_location=device, weights_only=True))\n",
        "    model.eval()\n",
        "\n",
        "    if model_type == 'raw model':\n",
        "        return model\n",
        "    if model_type == 'scripted model':\n",
        "        return torch.jit.script(model)\n",
        "    if model_type == 'traced model':\n",
        "        return torch.jit.trace(model, torch.rand((5, 3, 32, 32), device=device))\n",
        "    if model_type == 'frozen model':\n",
        "        return torch.jit.freeze(torch.jit.script(model))\n",
        "    if model_type == 'optimized for inference':\n",
        "        return torch.jit.optimize_for_inference(torch.jit.script(model))\n",
        "    if model_type == 'compiled model':\n",
        "        if os.name == 'nt':\n",
        "            print(\"torch.compile is not supported on Windows. Try Linux or WSL instead.\")\n",
        "            raise RuntimeError('windows')\n",
        "        return torch.compile(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "1aL8hVuunQbr",
      "metadata": {
        "id": "1aL8hVuunQbr"
      },
      "outputs": [],
      "source": [
        "def predict(device: torch.device = get_default_device(), data_path: str = './data',\n",
        "            checkpoint_path: str = \"./checkpoints\"):\n",
        "    if device.type == 'cuda':\n",
        "        torch.backends.cudnn.benchmark = True\n",
        "        torch.backends.cuda.matmul.allow_tf32 = True\n",
        "\n",
        "    val_dataset = get_dataset(data_path, is_train=False)\n",
        "    val_batch_size = 500\n",
        "    val_loader = DataLoader(val_dataset, shuffle=False, pin_memory=True, num_workers=0, batch_size=val_batch_size,\n",
        "                            drop_last=False)\n",
        "\n",
        "    use_tta = (False, True)\n",
        "    dtypes = (torch.bfloat16, torch.half, torch.float32) if device.type == 'cuda' else (torch.float32,)\n",
        "    model_types = (\n",
        "        'raw model', 'scripted model', 'traced model', 'frozen model', 'optimized for inference', 'compiled model')\n",
        "\n",
        "    for tta in use_tta:\n",
        "        for dtype in dtypes:\n",
        "            for model_type in model_types:\n",
        "                inference_mode = True\n",
        "                if model_type == 'compiled model':\n",
        "                    # On google colab, torch.compile might not like torch.inference_mode and wants torch.no_grad instead\n",
        "                    inference_mode = False\n",
        "                try:\n",
        "                    model = create_model(device, checkpoint_path, model_type)\n",
        "                    acc_val, elapsed = infer(\n",
        "                        model, val_loader, device, tta=tta, dtype=dtype, inference_mode=inference_mode)\n",
        "\n",
        "                    print(f\"Device {device.type}, val acc: {acc_val}, tta: {tta}, dtype: {dtype}, model type: {model_type}, \"\n",
        "                          f\"took: {elapsed / 1e9}s\")\n",
        "                except Exception as _:\n",
        "                    # Debug only\n",
        "                    # import traceback\n",
        "                    # traceback.print_exc()\n",
        "                    # print()\n",
        "\n",
        "                    print(f\"Model type {model_type} failed on {dtype} on {device.type}\")\n",
        "            print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "eysggQ5BpphA",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eysggQ5BpphA",
        "outputId": "2a8fce1c-3cec-4bf7-a141-d49b3aa85791"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-10-09 08:44:54--  https://github.com/Tensor-Reloaded/Advanced-Topics-in-Neural-Networks-Template-2024/raw/refs/heads/main/Lab02/CIFAR10/checkpoints/best.pth\n",
            "Resolving github.com (github.com)... 140.82.114.4\n",
            "Connecting to github.com (github.com)|140.82.114.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/Tensor-Reloaded/Advanced-Topics-in-Neural-Networks-Template-2024/refs/heads/main/Lab02/CIFAR10/checkpoints/best.pth [following]\n",
            "--2024-10-09 08:44:55--  https://raw.githubusercontent.com/Tensor-Reloaded/Advanced-Topics-in-Neural-Networks-Template-2024/refs/heads/main/Lab02/CIFAR10/checkpoints/best.pth\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 44739432 (43M) [application/octet-stream]\n",
            "Saving to: ‘best.pth’\n",
            "\n",
            "best.pth            100%[===================>]  42.67M   111MB/s    in 0.4s    \n",
            "\n",
            "2024-10-09 08:44:56 (111 MB/s) - ‘best.pth’ saved [44739432/44739432]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://github.com/Tensor-Reloaded/Advanced-Topics-in-Neural-Networks-Template-2024/raw/refs/heads/main/Lab02/CIFAR10/checkpoints/best.pth -O best.pth"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Results 1 (Google Colab): Tesla T4, Intel(R) Xeon(R) CPU @ 2.20GHz (1 core)"
      ],
      "metadata": {
        "id": "0-ZBtZ9lyRRC"
      },
      "id": "0-ZBtZ9lyRRC"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "cnK9oSepp0C_",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cnK9oSepp0C_",
        "outputId": "622e85cf-4cee-4d20-f95a-4e2ac01cfcdd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:04<00:00, 40081480.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Device cpu, val acc: 0.937, tta: False, dtype: torch.float32, model type: raw model, took: 155.261570115s\n",
            "Device cpu, val acc: 0.937, tta: False, dtype: torch.float32, model type: scripted model, took: 141.854798342s\n",
            "Device cpu, val acc: 0.937, tta: False, dtype: torch.float32, model type: traced model, took: 137.268522927s\n",
            "Device cpu, val acc: 0.937, tta: False, dtype: torch.float32, model type: frozen model, took: 139.033478704s\n",
            "Device cpu, val acc: 0.937, tta: False, dtype: torch.float32, model type: optimized for inference, took: 122.189013241s\n",
            "Device cpu, val acc: 0.937, tta: False, dtype: torch.float32, model type: compiled model, took: 155.985327564s\n",
            "\n",
            "Device cpu, val acc: 0.9425, tta: True, dtype: torch.float32, model type: raw model, took: 604.946253624s\n",
            "Device cpu, val acc: 0.9425, tta: True, dtype: torch.float32, model type: scripted model, took: 605.447234843s\n",
            "Device cpu, val acc: 0.9425, tta: True, dtype: torch.float32, model type: traced model, took: 596.920129333s\n",
            "Device cpu, val acc: 0.9425, tta: True, dtype: torch.float32, model type: frozen model, took: 584.127071722s\n",
            "Device cpu, val acc: 0.9425, tta: True, dtype: torch.float32, model type: optimized for inference, took: 493.994507287s\n",
            "Device cpu, val acc: 0.9425, tta: True, dtype: torch.float32, model type: compiled model, took: 475.12032245s\n",
            "\n"
          ]
        }
      ],
      "source": [
        "predict(checkpoint_path='./', device=torch.device('cpu'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "FX7zvpS4p5gt",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FX7zvpS4p5gt",
        "outputId": "41f7159f-70c1-4a1d-8956-01a7f6ef1800"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Device cuda, val acc: 0.9369, tta: False, dtype: torch.bfloat16, model type: raw model, took: 35.6871717s\n",
            "Device cuda, val acc: 0.9369, tta: False, dtype: torch.bfloat16, model type: scripted model, took: 35.490419012s\n",
            "Device cuda, val acc: 0.9369, tta: False, dtype: torch.bfloat16, model type: traced model, took: 35.465361561s\n",
            "Device cuda, val acc: 0.9366, tta: False, dtype: torch.bfloat16, model type: frozen model, took: 35.363827883s\n",
            "Model type optimized for inference failed on torch.bfloat16 on cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py:1607: UserWarning: Tesla T4 does not support bfloat16 compilation natively, skipping\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py:1607: UserWarning: Tesla T4 does not support bfloat16 compilation natively, skipping\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device cuda, val acc: 0.9369, tta: False, dtype: torch.bfloat16, model type: compiled model, took: 38.592140568s\n",
            "\n",
            "Device cuda, val acc: 0.937, tta: False, dtype: torch.float16, model type: raw model, took: 1.63877895s\n",
            "Device cuda, val acc: 0.9369, tta: False, dtype: torch.float16, model type: scripted model, took: 1.056358059s\n",
            "Device cuda, val acc: 0.937, tta: False, dtype: torch.float16, model type: traced model, took: 1.106966217s\n",
            "Device cuda, val acc: 0.937, tta: False, dtype: torch.float16, model type: frozen model, took: 1.096997849s\n",
            "Model type optimized for inference failed on torch.float16 on cuda\n",
            "Device cuda, val acc: 0.937, tta: False, dtype: torch.float16, model type: compiled model, took: 1.058264296s\n",
            "\n",
            "Device cuda, val acc: 0.937, tta: False, dtype: torch.float32, model type: raw model, took: 3.504440625s\n",
            "Device cuda, val acc: 0.937, tta: False, dtype: torch.float32, model type: scripted model, took: 2.057086411s\n",
            "Device cuda, val acc: 0.937, tta: False, dtype: torch.float32, model type: traced model, took: 2.071086302s\n",
            "Device cuda, val acc: 0.937, tta: False, dtype: torch.float32, model type: frozen model, took: 2.106855646s\n",
            "Device cuda, val acc: 0.937, tta: False, dtype: torch.float32, model type: optimized for inference, took: 3.304816139s\n",
            "Device cuda, val acc: 0.937, tta: False, dtype: torch.float32, model type: compiled model, took: 2.057779591s\n",
            "\n",
            "Device cuda, val acc: 0.9423, tta: True, dtype: torch.bfloat16, model type: raw model, took: 141.036271387s\n",
            "Device cuda, val acc: 0.9423, tta: True, dtype: torch.bfloat16, model type: scripted model, took: 141.016564092s\n",
            "Device cuda, val acc: 0.9423, tta: True, dtype: torch.bfloat16, model type: traced model, took: 141.061894583s\n",
            "Device cuda, val acc: 0.9422, tta: True, dtype: torch.bfloat16, model type: frozen model, took: 140.926919031s\n",
            "Model type optimized for inference failed on torch.bfloat16 on cuda\n",
            "Device cuda, val acc: 0.9423, tta: True, dtype: torch.bfloat16, model type: compiled model, took: 141.017453895s\n",
            "\n",
            "Device cuda, val acc: 0.9425, tta: True, dtype: torch.float16, model type: raw model, took: 4.054621788s\n",
            "Device cuda, val acc: 0.9427, tta: True, dtype: torch.float16, model type: scripted model, took: 4.104409881s\n",
            "Device cuda, val acc: 0.9425, tta: True, dtype: torch.float16, model type: traced model, took: 4.120940505s\n",
            "Device cuda, val acc: 0.9424, tta: True, dtype: torch.float16, model type: frozen model, took: 4.097629869s\n",
            "Model type optimized for inference failed on torch.float16 on cuda\n",
            "Device cuda, val acc: 0.9425, tta: True, dtype: torch.float16, model type: compiled model, took: 4.068377594s\n",
            "\n",
            "Device cuda, val acc: 0.9425, tta: True, dtype: torch.float32, model type: raw model, took: 8.09819478s\n",
            "Device cuda, val acc: 0.9425, tta: True, dtype: torch.float32, model type: scripted model, took: 8.095883311s\n",
            "Device cuda, val acc: 0.9425, tta: True, dtype: torch.float32, model type: traced model, took: 8.126018216s\n",
            "Device cuda, val acc: 0.9425, tta: True, dtype: torch.float32, model type: frozen model, took: 8.135016707s\n",
            "Device cuda, val acc: 0.9425, tta: True, dtype: torch.float32, model type: optimized for inference, took: 7.603647899s\n",
            "Device cuda, val acc: 0.9425, tta: True, dtype: torch.float32, model type: compiled model, took: 8.102612386s\n",
            "\n"
          ]
        }
      ],
      "source": [
        "predict(checkpoint_path='./', device=torch.device('cuda:0'))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Results 2: NVIDIA A40, Intel(R) Xeon(R) Silver 4310T CPU @ 2.30GHz\n"
      ],
      "metadata": {
        "id": "32c5ZGXSynKB"
      },
      "id": "32c5ZGXSynKB"
    },
    {
      "cell_type": "code",
      "source": [
        "# CPU\n",
        "\n",
        "Files already downloaded and verified\n",
        "Device cpu, val acc: 0.937, tta: False, dtype: torch.float32, model type: raw model, took: 29.807860215s\n",
        "Device cpu, val acc: 0.937, tta: False, dtype: torch.float32, model type: scripted model, took: 30.61111084s\n",
        "Device cpu, val acc: 0.937, tta: False, dtype: torch.float32, model type: traced model, took: 29.401212288s\n",
        "Device cpu, val acc: 0.937, tta: False, dtype: torch.float32, model type: frozen model, took: 27.164150199s\n",
        "Device cpu, val acc: 0.937, tta: False, dtype: torch.float32, model type: optimized for inference, took: 19.921779904s\n",
        "Device cpu, val acc: 0.937, tta: False, dtype: torch.float32, model type: compiled model, took: 18.938307475s\n",
        "\n",
        "Device cpu, val acc: 0.9425, tta: True, dtype: torch.float32, model type: raw model, took: 118.477152989s\n",
        "Device cpu, val acc: 0.9425, tta: True, dtype: torch.float32, model type: scripted model, took: 118.600493398s\n",
        "Device cpu, val acc: 0.9425, tta: True, dtype: torch.float32, model type: traced model, took: 119.661402172s\n",
        "Device cpu, val acc: 0.9425, tta: True, dtype: torch.float32, model type: frozen model, took: 109.021421908s\n",
        "Device cpu, val acc: 0.9425, tta: True, dtype: torch.float32, model type: optimized for inference, took: 80.80285353s\n",
        "Device cpu, val acc: 0.9425, tta: True, dtype: torch.float32, model type: compiled model, took: 62.447971997s"
      ],
      "metadata": {
        "id": "MVUKKID_y9hs"
      },
      "id": "MVUKKID_y9hs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GPU\n",
        "\n",
        "Files already downloaded and verified\n",
        "Device cuda, val acc: 0.9369, tta: False, dtype: torch.bfloat16, model type: raw model, took: 1.662712012s\n",
        "Device cuda, val acc: 0.9369, tta: False, dtype: torch.bfloat16, model type: scripted model, took: 0.555742949s\n",
        "Device cuda, val acc: 0.9369, tta: False, dtype: torch.bfloat16, model type: traced model, took: 0.473243101s\n",
        "Device cuda, val acc: 0.9369, tta: False, dtype: torch.bfloat16, model type: frozen model, took: 0.430498959s\n",
        "Model type optimized for inference failed on torch.bfloat16 on cuda\n",
        "Device cuda, val acc: 0.9369, tta: False, dtype: torch.bfloat16, model type: compiled model, took: 7.265515185s\n",
        "\n",
        "Device cuda, val acc: 0.9367, tta: False, dtype: torch.float16, model type: raw model, took: 0.802255959s\n",
        "Device cuda, val acc: 0.9369, tta: False, dtype: torch.float16, model type: scripted model, took: 0.367570652s\n",
        "Device cuda, val acc: 0.9367, tta: False, dtype: torch.float16, model type: traced model, took: 0.438119956s\n",
        "Device cuda, val acc: 0.9368, tta: False, dtype: torch.float16, model type: frozen model, took: 0.411527476s\n",
        "Model type optimized for inference failed on torch.float16 on cuda\n",
        "Device cuda, val acc: 0.9369, tta: False, dtype: torch.float16, model type: compiled model, took: 3.105313892s\n",
        "\n",
        "Device cuda, val acc: 0.937, tta: False, dtype: torch.float32, model type: raw model, took: 0.788350018s\n",
        "Device cuda, val acc: 0.937, tta: False, dtype: torch.float32, model type: scripted model, took: 0.642222682s\n",
        "Device cuda, val acc: 0.937, tta: False, dtype: torch.float32, model type: traced model, took: 0.582183428s\n",
        "Device cuda, val acc: 0.937, tta: False, dtype: torch.float32, model type: frozen model, took: 0.63175085s\n",
        "Device cuda, val acc: 0.937, tta: False, dtype: torch.float32, model type: optimized for inference, took: 0.860065195s\n",
        "Device cuda, val acc: 0.9369, tta: False, dtype: torch.float32, model type: compiled model, took: 3.26706575s\n",
        "\n",
        "Device cuda, val acc: 0.9423, tta: True, dtype: torch.bfloat16, model type: raw model, took: 1.29290948s\n",
        "Device cuda, val acc: 0.9423, tta: True, dtype: torch.bfloat16, model type: scripted model, took: 1.310474388s\n",
        "Device cuda, val acc: 0.9423, tta: True, dtype: torch.bfloat16, model type: traced model, took: 1.329242116s\n",
        "Device cuda, val acc: 0.9424, tta: True, dtype: torch.bfloat16, model type: frozen model, took: 1.313594689s\n",
        "Model type optimized for inference failed on torch.bfloat16 on cuda\n",
        "Device cuda, val acc: 0.9422, tta: True, dtype: torch.bfloat16, model type: compiled model, took: 3.165473544s\n",
        "\n",
        "Device cuda, val acc: 0.9426, tta: True, dtype: torch.float16, model type: raw model, took: 1.289959265s\n",
        "Device cuda, val acc: 0.9425, tta: True, dtype: torch.float16, model type: scripted model, took: 1.29710779s\n",
        "Device cuda, val acc: 0.9426, tta: True, dtype: torch.float16, model type: traced model, took: 1.320457467s\n",
        "Device cuda, val acc: 0.9426, tta: True, dtype: torch.float16, model type: frozen model, took: 1.312074838s\n",
        "Model type optimized for inference failed on torch.float16 on cuda\n",
        "Device cuda, val acc: 0.9426, tta: True, dtype: torch.float16, model type: compiled model, took: 3.2982492s\n",
        "\n",
        "Device cuda, val acc: 0.9426, tta: True, dtype: torch.float32, model type: raw model, took: 2.130355637s\n",
        "Device cuda, val acc: 0.9426, tta: True, dtype: torch.float32, model type: scripted model, took: 2.137523323s\n",
        "Device cuda, val acc: 0.9426, tta: True, dtype: torch.float32, model type: traced model, took: 2.161863139s\n",
        "Device cuda, val acc: 0.9425, tta: True, dtype: torch.float32, model type: frozen model, took: 2.148259623s\n",
        "Device cuda, val acc: 0.9425, tta: True, dtype: torch.float32, model type: optimized for inference, took: 1.885170125s\n",
        "Device cuda, val acc: 0.9426, tta: True, dtype: torch.float32, model type: compiled model, took: 3.971305783s"
      ],
      "metadata": {
        "id": "zNxKLTyJy_Hg"
      },
      "id": "zNxKLTyJy_Hg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Results 3: NVIDIA GeForce RTX 3090, AMD Ryzen Threadripper 1920X 12-Core Processor"
      ],
      "metadata": {
        "id": "SgnPneoazPyL"
      },
      "id": "SgnPneoazPyL"
    },
    {
      "cell_type": "code",
      "source": [
        "# CPU\n",
        "\n",
        "Files already downloaded and verified\n",
        "Device cpu, val acc: 0.937, tta: False, dtype: torch.float32, model type: raw model, took: 35.64374242s\n",
        "Device cpu, val acc: 0.937, tta: False, dtype: torch.float32, model type: scripted model, took: 34.561748777s\n",
        "Device cpu, val acc: 0.937, tta: False, dtype: torch.float32, model type: traced model, took: 34.939105964s\n",
        "Device cpu, val acc: 0.937, tta: False, dtype: torch.float32, model type: frozen model, took: 34.126268667s\n",
        "Device cpu, val acc: 0.937, tta: False, dtype: torch.float32, model type: optimized for inference, took: 26.412186696s\n",
        "Device cpu, val acc: 0.937, tta: False, dtype: torch.float32, model type: compiled model, took: 31.803454048s\n",
        "\n",
        "Device cpu, val acc: 0.9425, tta: True, dtype: torch.float32, model type: raw model, took: 141.637965004s\n",
        "Device cpu, val acc: 0.9425, tta: True, dtype: torch.float32, model type: scripted model, took: 140.86321628s\n",
        "Device cpu, val acc: 0.9425, tta: True, dtype: torch.float32, model type: traced model, took: 140.037531754s\n",
        "Device cpu, val acc: 0.9425, tta: True, dtype: torch.float32, model type: frozen model, took: 133.869568961s\n",
        "Device cpu, val acc: 0.9425, tta: True, dtype: torch.float32, model type: optimized for inference, took: 106.60755021s\n",
        "Device cpu, val acc: 0.9425, tta: True, dtype: torch.float32, model type: compiled model, took: 95.432965662s"
      ],
      "metadata": {
        "id": "-Iv7DHGyzWXT"
      },
      "id": "-Iv7DHGyzWXT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GPU\n",
        "\n",
        "Files already downloaded and verified\n",
        "Device cuda, val acc: 0.9369, tta: False, dtype: torch.bfloat16, model type: raw model, took: 1.045950173s\n",
        "Device cuda, val acc: 0.9369, tta: False, dtype: torch.bfloat16, model type: scripted model, took: 0.507382365s\n",
        "Device cuda, val acc: 0.9369, tta: False, dtype: torch.bfloat16, model type: traced model, took: 0.483161688s\n",
        "Device cuda, val acc: 0.9369, tta: False, dtype: torch.bfloat16, model type: frozen model, took: 0.487306687s\n",
        "Model type optimized for inference failed on torch.bfloat16 on cuda\n",
        "Device cuda, val acc: 0.9369, tta: False, dtype: torch.bfloat16, model type: compiled model, took: 8.428094218s\n",
        "\n",
        "Device cuda, val acc: 0.937, tta: False, dtype: torch.float16, model type: raw model, took: 0.755911188s\n",
        "Device cuda, val acc: 0.9369, tta: False, dtype: torch.float16, model type: scripted model, took: 0.437476219s\n",
        "Device cuda, val acc: 0.937, tta: False, dtype: torch.float16, model type: traced model, took: 0.466745924s\n",
        "Device cuda, val acc: 0.9368, tta: False, dtype: torch.float16, model type: frozen model, took: 0.453104697s\n",
        "Model type optimized for inference failed on torch.float16 on cuda\n",
        "Device cuda, val acc: 0.9368, tta: False, dtype: torch.float16, model type: compiled model, took: 5.717307314s\n",
        "\n",
        "Device cuda, val acc: 0.9369, tta: False, dtype: torch.float32, model type: raw model, took: 0.930242167s\n",
        "Device cuda, val acc: 0.9369, tta: False, dtype: torch.float32, model type: scripted model, took: 0.646624205s\n",
        "Device cuda, val acc: 0.9369, tta: False, dtype: torch.float32, model type: traced model, took: 0.66108906s\n",
        "Device cuda, val acc: 0.9368, tta: False, dtype: torch.float32, model type: frozen model, took: 0.672550878s\n",
        "Device cuda, val acc: 0.9369, tta: False, dtype: torch.float32, model type: optimized for inference, took: 1.047787336s\n",
        "Device cuda, val acc: 0.9369, tta: False, dtype: torch.float32, model type: compiled model, took: 5.533834633s\n",
        "\n",
        "Device cuda, val acc: 0.9423, tta: True, dtype: torch.bfloat16, model type: raw model, took: 1.381747827s\n",
        "Device cuda, val acc: 0.9423, tta: True, dtype: torch.bfloat16, model type: scripted model, took: 1.381004199s\n",
        "Device cuda, val acc: 0.9423, tta: True, dtype: torch.bfloat16, model type: traced model, took: 1.419552829s\n",
        "Device cuda, val acc: 0.9423, tta: True, dtype: torch.bfloat16, model type: frozen model, took: 1.388475839s\n",
        "Model type optimized for inference failed on torch.bfloat16 on cuda\n",
        "Device cuda, val acc: 0.9423, tta: True, dtype: torch.bfloat16, model type: compiled model, took: 3.802048509s\n",
        "\n",
        "Device cuda, val acc: 0.9426, tta: True, dtype: torch.float16, model type: raw model, took: 1.389953422s\n",
        "Device cuda, val acc: 0.9425, tta: True, dtype: torch.float16, model type: scripted model, took: 1.405247425s\n",
        "Device cuda, val acc: 0.9426, tta: True, dtype: torch.float16, model type: traced model, took: 1.424549875s\n",
        "Device cuda, val acc: 0.9426, tta: True, dtype: torch.float16, model type: frozen model, took: 1.395574333s\n",
        "Model type optimized for inference failed on torch.float16 on cuda\n",
        "Device cuda, val acc: 0.9426, tta: True, dtype: torch.float16, model type: compiled model, took: 3.865965502s\n",
        "\n",
        "Device cuda, val acc: 0.9426, tta: True, dtype: torch.float32, model type: raw model, took: 2.163474659s\n",
        "Device cuda, val acc: 0.9426, tta: True, dtype: torch.float32, model type: scripted model, took: 2.210117595s\n",
        "Device cuda, val acc: 0.9426, tta: True, dtype: torch.float32, model type: traced model, took: 2.17419933s\n",
        "Device cuda, val acc: 0.9425, tta: True, dtype: torch.float32, model type: frozen model, took: 2.234084868s\n",
        "Device cuda, val acc: 0.9426, tta: True, dtype: torch.float32, model type: optimized for inference, took: 2.144272799s\n",
        "Device cuda, val acc: 0.9426, tta: True, dtype: torch.float32, model type: compiled model, took: 4.714134742s"
      ],
      "metadata": {
        "id": "kG7kUvAqzaBP"
      },
      "id": "kG7kUvAqzaBP",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}