{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "nhbNpDWGnjG_",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nhbNpDWGnjG_",
    "outputId": "1bef38f9-1c40-41fd-84ab-7417da33ad84"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: timed-decorator in /opt/conda/lib/python3.12/site-packages (1.5.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install timed-decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8H-nbkBWnlc6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8H-nbkBWnlc6",
    "outputId": "204b4091-0e41-4fb7-faad-526b90297449"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: triton in /opt/conda/lib/python3.12/site-packages (3.0.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.12/site-packages (from triton) (3.15.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install triton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "-PAsUAbZnftE",
   "metadata": {
    "id": "-PAsUAbZnftE"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from multiprocessing import freeze_support\n",
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from timed_decorator.simple_timed import timed\n",
    "from torch import Tensor, nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision.transforms import v2\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "oh1MgCN1ngwF",
   "metadata": {
    "id": "oh1MgCN1ngwF"
   },
   "outputs": [],
   "source": [
    "def get_default_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    if torch.backends.mps.is_available():\n",
    "        return torch.device('mps')\n",
    "    return torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "E0dqEsC4npOQ",
   "metadata": {
    "id": "E0dqEsC4npOQ"
   },
   "outputs": [],
   "source": [
    "class PreActBlock(nn.Module):\n",
    "    \"\"\"Pre-activation version of the BasicBlock.\"\"\"\n",
    "\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(PreActBlock, self).__init__()\n",
    "        self.bn1 = nn.BatchNorm2d(in_planes)\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False\n",
    "        )\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            planes, planes, kernel_size=3, stride=1, padding=1, bias=False\n",
    "        )\n",
    "\n",
    "        if stride != 1 or in_planes != self.expansion * planes:\n",
    "            self.shortcut = nn.Conv2d(\n",
    "                in_planes,\n",
    "                self.expansion * planes,\n",
    "                kernel_size=1,\n",
    "                stride=stride,\n",
    "                bias=False,\n",
    "            )\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        out = F.relu(self.bn1(x), inplace=True)\n",
    "        shortcut = self.shortcut(out) if hasattr(self, \"shortcut\") else x\n",
    "        out = self.conv1(out)\n",
    "        out = self.conv2(F.relu(self.bn2(out), inplace=True))\n",
    "        out += shortcut\n",
    "        return out\n",
    "\n",
    "\n",
    "class PreActBottleneck(nn.Module):\n",
    "    \"\"\"Pre-activation version of the original Bottleneck module.\"\"\"\n",
    "\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(PreActBottleneck, self).__init__()\n",
    "        self.bn1 = nn.BatchNorm2d(in_planes)\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            planes, planes, kernel_size=3, stride=stride, padding=1, bias=False\n",
    "        )\n",
    "        self.bn3 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(\n",
    "            planes, self.expansion * planes, kernel_size=1, bias=False\n",
    "        )\n",
    "\n",
    "        if stride != 1 or in_planes != self.expansion * planes:\n",
    "            self.shortcut = nn.Conv2d(\n",
    "                in_planes,\n",
    "                self.expansion * planes,\n",
    "                kernel_size=1,\n",
    "                stride=stride,\n",
    "                bias=False,\n",
    "            )\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        out = F.relu(self.bn1(x), inplace=True)\n",
    "        shortcut = self.shortcut(out) if hasattr(self, \"shortcut\") else x\n",
    "        out = self.conv1(out)\n",
    "        out = self.conv2(F.relu(self.bn2(out), inplace=True))\n",
    "        out = self.conv3(F.relu(self.bn3(out), inplace=True))\n",
    "        out += shortcut\n",
    "        return out\n",
    "\n",
    "\n",
    "class PreActResNet_C10(nn.Module):\n",
    "    \"\"\"Pre-activation ResNet for CIFAR-10\"\"\"\n",
    "\n",
    "    def __init__(self, block, num_blocks, num_classes):\n",
    "        super(PreActResNet_C10, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        out = self.conv1(x)\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def PreActResNet18_C10(num_classes):\n",
    "    return PreActResNet_C10(PreActBlock, [2, 2, 2, 2], num_classes)\n",
    "\n",
    "\n",
    "def get_model():\n",
    "    return PreActResNet18_C10(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ga0_rh4sns61",
   "metadata": {
    "id": "ga0_rh4sns61"
   },
   "outputs": [],
   "source": [
    "class CachedDataset(Dataset):\n",
    "    def __init__(self, dataset: Dataset, runtime_transforms: Optional[v2.Transform], cache: bool):\n",
    "        if cache:\n",
    "            dataset = tuple([x for x in dataset])\n",
    "        self.dataset = dataset\n",
    "        self.runtime_transforms = runtime_transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        image, label = self.dataset[i]\n",
    "        if self.runtime_transforms is None:\n",
    "            return image, label\n",
    "        return self.runtime_transforms(image), label\n",
    "\n",
    "def get_dataset(data_path: str, is_train: bool):\n",
    "    initial_transforms = v2.Compose([\n",
    "        v2.ToImage(),\n",
    "        v2.ToDtype(torch.float32, scale=True),\n",
    "        v2.Normalize(\n",
    "            mean=(0.491, 0.482, 0.446),\n",
    "            std=(0.247, 0.243, 0.261)\n",
    "        ),\n",
    "    ])\n",
    "    cifar10 = CIFAR10(root=data_path, train=is_train, transform=initial_transforms, download=True)\n",
    "    runtime_transforms = None\n",
    "    if is_train:\n",
    "        runtime_transforms = v2.Compose([\n",
    "            v2.RandomCrop(size=32, padding=4),\n",
    "            v2.RandomHorizontalFlip(),\n",
    "            v2.RandomVerticalFlip(),\n",
    "            v2.RandomErasing()\n",
    "        ])\n",
    "    return CachedDataset(cifar10, runtime_transforms, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "XbmFnPgTnNC5",
   "metadata": {
    "id": "XbmFnPgTnNC5"
   },
   "outputs": [],
   "source": [
    "@torch.jit.script\n",
    "def accuracy(output: Tensor, labels: Tensor):\n",
    "    fp_plus_fn = torch.logical_not(output == labels).sum().item()\n",
    "    all_elements = len(output)\n",
    "    return (all_elements - fp_plus_fn) / all_elements\n",
    "\n",
    "\n",
    "def train(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "\n",
    "    all_outputs = []\n",
    "    all_labels = []\n",
    "\n",
    "    for data, labels in train_loader:\n",
    "        data = data.to(device, non_blocking=True)\n",
    "        labels = labels.to(device, non_blocking=True)\n",
    "        output = model(data)\n",
    "        loss = criterion(output, labels)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        # torch.nn.utils.clip_grad_norm_(model.parameters(), 5)\n",
    "\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        output = output.softmax(dim=1).detach().cpu().squeeze()\n",
    "        labels = labels.cpu().squeeze()\n",
    "        all_outputs.append(output)\n",
    "        all_labels.append(labels)\n",
    "\n",
    "    all_outputs = torch.cat(all_outputs).argmax(dim=1)\n",
    "    all_labels = torch.cat(all_labels)\n",
    "\n",
    "    return round(accuracy(all_outputs, all_labels), 4)\n",
    "\n",
    "\n",
    "@torch.inference_mode()\n",
    "def val(model, val_loader, device):\n",
    "    model.eval()\n",
    "\n",
    "    all_outputs = []\n",
    "    all_labels = []\n",
    "\n",
    "    for data, labels in val_loader:\n",
    "        data = data.to(device, non_blocking=True)\n",
    "        output = model(data)\n",
    "\n",
    "        output = output.softmax(dim=1).cpu().squeeze()\n",
    "        labels = labels.squeeze()\n",
    "        all_outputs.append(output)\n",
    "        all_labels.append(labels)\n",
    "\n",
    "    all_outputs = torch.cat(all_outputs).argmax(dim=1)\n",
    "    all_labels = torch.cat(all_labels)\n",
    "\n",
    "    return round(accuracy(all_outputs, all_labels), 4)\n",
    "\n",
    "\n",
    "def do_epoch(model, train_loader, val_loader, criterion, optimizer, device):\n",
    "    acc = train(model, train_loader, criterion, optimizer, device)\n",
    "    acc_val = val(model, val_loader, device)\n",
    "    # torch.cuda.empty_cache()\n",
    "    return acc, acc_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "Bd7kyNEJn2_F",
   "metadata": {
    "id": "Bd7kyNEJn2_F"
   },
   "outputs": [],
   "source": [
    "def main(device: torch.device = get_default_device(), data_path: str = './data',\n",
    "         checkpoint_path: str = \"./checkpoints\"):\n",
    "    print(f\"Using {device}\")\n",
    "    os.makedirs(checkpoint_path, exist_ok=True)\n",
    "    if device.type == 'cuda':\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "    train_dataset = get_dataset(data_path, is_train=True)\n",
    "    val_dataset = get_dataset(data_path, is_train=False)\n",
    "\n",
    "    model = get_model()\n",
    "    model = model.to(device)\n",
    "    model = torch.jit.script(model)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9, nesterov=True, weight_decay=0.00001,\n",
    "                                fused=True)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.2, patience=10,\n",
    "                                                           threshold=0.001, threshold_mode='rel')\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    batch_size = 50\n",
    "    val_batch_size = 500\n",
    "    num_workers = 0\n",
    "    persistent_workers = (num_workers != 0) and False\n",
    "    train_loader = DataLoader(train_dataset, shuffle=True, pin_memory=(device.type == 'cuda'), num_workers=num_workers,\n",
    "                              batch_size=batch_size, drop_last=True, persistent_workers=persistent_workers)\n",
    "    val_loader = DataLoader(val_dataset, shuffle=False, pin_memory=True, num_workers=0, batch_size=val_batch_size,\n",
    "                            drop_last=False)\n",
    "\n",
    "    epochs = tuple(range(200))\n",
    "    best_val = 0.0\n",
    "    with tqdm(epochs) as tbar:\n",
    "        for _ in tbar:\n",
    "            acc, acc_val = do_epoch(model, train_loader, val_loader, criterion, optimizer, device)\n",
    "            scheduler.step(acc)\n",
    "\n",
    "            if acc_val > best_val:\n",
    "                torch.save(model.state_dict(), os.path.join(checkpoint_path, \"best.pth\"))\n",
    "                best_val = acc_val\n",
    "            tbar.set_description(f\"Acc: {acc}, Acc_val: {acc_val}, Best_val: {best_val}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "RAhy_VyEndMH",
   "metadata": {
    "id": "RAhy_VyEndMH"
   },
   "outputs": [],
   "source": [
    "@timed(stdout=False, return_time=True)\n",
    "def infer(model, val_loader, device, tta, dtype, inference_mode):\n",
    "    model.eval()\n",
    "    all_outputs = []\n",
    "    all_labels = []\n",
    "\n",
    "    inference_mode = torch.inference_mode if inference_mode else torch.no_grad\n",
    "\n",
    "    enable_autocast = device.type != 'cpu' and dtype != torch.float32\n",
    "    # Autocast is slow for cpu, so we disable it.\n",
    "    # Also, if the device type is mps, autocast might not work (?) and disabling it might also not work (?)\n",
    "    with torch.autocast(device_type=device.type, dtype=dtype, enabled=enable_autocast), inference_mode():\n",
    "        for data, labels in val_loader:\n",
    "            data = data.to(device, non_blocking=True)\n",
    "\n",
    "            output = model(data)\n",
    "            if tta:\n",
    "                # Horizontal rotation:\n",
    "                output += model(v2.functional.hflip(data))\n",
    "                # Vertical rotation:\n",
    "                output += model(v2.functional.vflip(data))\n",
    "                # Horizontal rotation + Vertical rotation:\n",
    "                output += model(v2.functional.hflip(v2.functional.vflip(data)))\n",
    "\n",
    "            output = output.softmax(dim=1).cpu().squeeze()\n",
    "            labels = labels.squeeze()\n",
    "            all_outputs.append(output)\n",
    "            all_labels.append(labels)\n",
    "\n",
    "    all_outputs = torch.cat(all_outputs).argmax(dim=1)\n",
    "    all_labels = torch.cat(all_labels)\n",
    "\n",
    "    return round(accuracy(all_outputs, all_labels), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "BQt0O-lanWbZ",
   "metadata": {
    "id": "BQt0O-lanWbZ"
   },
   "outputs": [],
   "source": [
    "def create_model(device: torch.device, checkpoint_path: str, model_type: str):\n",
    "    model = get_model()\n",
    "    model = model.to(device)\n",
    "    model.load_state_dict(torch.load(os.path.join(checkpoint_path, \"best.pth\"), map_location=device, weights_only=True))\n",
    "    model.eval()\n",
    "\n",
    "    if model_type == 'raw model':\n",
    "        return model\n",
    "    if model_type == 'scripted model':\n",
    "        return torch.jit.script(model)\n",
    "    if model_type == 'traced model':\n",
    "        return torch.jit.trace(model, torch.rand((5, 3, 32, 32), device=device))\n",
    "    if model_type == 'frozen model':\n",
    "        return torch.jit.freeze(torch.jit.script(model))\n",
    "    if model_type == 'optimized for inference':\n",
    "        return torch.jit.optimize_for_inference(torch.jit.script(model))\n",
    "    if model_type == 'compiled model':\n",
    "        if os.name == 'nt':\n",
    "            print(\"torch.compile is not supported on Windows. Try Linux or WSL instead.\")\n",
    "            raise RuntimeError('windows')\n",
    "        return torch.compile(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1aL8hVuunQbr",
   "metadata": {
    "id": "1aL8hVuunQbr"
   },
   "outputs": [],
   "source": [
    "def predict(device: torch.device = get_default_device(), data_path: str = './data',\n",
    "            checkpoint_path: str = \"./checkpoints\"):\n",
    "    if device.type == 'cuda':\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "    val_dataset = get_dataset(data_path, is_train=False)\n",
    "    val_batch_size = 500\n",
    "    val_loader = DataLoader(val_dataset, shuffle=False, pin_memory=True, num_workers=0, batch_size=val_batch_size,\n",
    "                            drop_last=False)\n",
    "\n",
    "    use_tta = (False, True)\n",
    "    dtypes = (torch.bfloat16, torch.half, torch.float32) if device.type == 'cuda' else (torch.float32,)\n",
    "    model_types = (\n",
    "        'raw model', 'scripted model', 'traced model', 'frozen model', 'optimized for inference', 'compiled model')\n",
    "\n",
    "    for tta in use_tta:\n",
    "        for dtype in dtypes:\n",
    "            for model_type in model_types:\n",
    "                inference_mode = True\n",
    "                if model_type == 'compiled model':\n",
    "                    # On google colab, torch.compile might not like torch.inference_mode and wants torch.no_grad instead\n",
    "                    inference_mode = False\n",
    "                try:\n",
    "                    model = create_model(device, checkpoint_path, model_type)\n",
    "                    acc_val, elapsed = infer(\n",
    "                        model, val_loader, device, tta=tta, dtype=dtype, inference_mode=inference_mode)\n",
    "\n",
    "                    print(f\"Device {device.type}, val acc: {acc_val}, tta: {tta}, dtype: {dtype}, model type: {model_type}, \"\n",
    "                          f\"took: {elapsed / 1e9}s\")\n",
    "                except Exception as _:\n",
    "                    # Debug only\n",
    "                    # import traceback\n",
    "                    # traceback.print_exc()\n",
    "                    # print()\n",
    "\n",
    "                    print(f\"Model type {model_type} failed on {dtype} on {device.type}\")\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eysggQ5BpphA",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eysggQ5BpphA",
    "outputId": "c01aec06-b4f9-4352-9068-de60690b96c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-10-08 11:00:50--  https://github.com/Tensor-Reloaded/Advanced-Topics-in-Neural-Networks-Template-2024/raw/refs/heads/main/Lab02/CIFAR10/checkpoints/best.pth\n",
      "Resolving github.com (github.com)... 140.82.121.4\n",
      "Connecting to github.com (github.com)|140.82.121.4|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://raw.githubusercontent.com/Tensor-Reloaded/Advanced-Topics-in-Neural-Networks-Template-2024/refs/heads/main/Lab02/CIFAR10/checkpoints/best.pth [following]\n",
      "--2024-10-08 11:00:51--  https://raw.githubusercontent.com/Tensor-Reloaded/Advanced-Topics-in-Neural-Networks-Template-2024/refs/heads/main/Lab02/CIFAR10/checkpoints/best.pth\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.109.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 44739432 (43M) [application/octet-stream]\n",
      "Saving to: ‘best.pth’\n",
      "\n",
      "best.pth            100%[===================>]  42.67M   100MB/s    in 0.4s    \n",
      "\n",
      "2024-10-08 11:00:51 (100 MB/s) - ‘best.pth’ saved [44739432/44739432]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://github.com/Tensor-Reloaded/Advanced-Topics-in-Neural-Networks-Template-2024/raw/refs/heads/main/Lab02/CIFAR10/checkpoints/best.pth -O best.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cnK9oSepp0C_",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "cnK9oSepp0C_",
    "outputId": "c0c4ec6a-a932-4831-ea4a-18e65d3cce80"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Device cpu, val acc: 0.937, tta: False, dtype: torch.float32, model type: raw model, took: 29.807860215s\n",
      "Device cpu, val acc: 0.937, tta: False, dtype: torch.float32, model type: scripted model, took: 30.61111084s\n",
      "Device cpu, val acc: 0.937, tta: False, dtype: torch.float32, model type: traced model, took: 29.401212288s\n",
      "Device cpu, val acc: 0.937, tta: False, dtype: torch.float32, model type: frozen model, took: 27.164150199s\n",
      "Device cpu, val acc: 0.937, tta: False, dtype: torch.float32, model type: optimized for inference, took: 19.921779904s\n",
      "Device cpu, val acc: 0.937, tta: False, dtype: torch.float32, model type: compiled model, took: 18.938307475s\n",
      "\n",
      "Device cpu, val acc: 0.9425, tta: True, dtype: torch.float32, model type: raw model, took: 118.477152989s\n",
      "Device cpu, val acc: 0.9425, tta: True, dtype: torch.float32, model type: scripted model, took: 118.600493398s\n",
      "Device cpu, val acc: 0.9425, tta: True, dtype: torch.float32, model type: traced model, took: 119.661402172s\n",
      "Device cpu, val acc: 0.9425, tta: True, dtype: torch.float32, model type: frozen model, took: 109.021421908s\n",
      "Device cpu, val acc: 0.9425, tta: True, dtype: torch.float32, model type: optimized for inference, took: 80.80285353s\n",
      "Device cpu, val acc: 0.9425, tta: True, dtype: torch.float32, model type: compiled model, took: 62.447971997s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predict(checkpoint_path='./', device=torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "FX7zvpS4p5gt",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FX7zvpS4p5gt",
    "outputId": "adde927a-8175-4ddd-ba59-05b332097b1c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Device cuda, val acc: 0.9369, tta: False, dtype: torch.bfloat16, model type: raw model, took: 1.757397702s\n",
      "Device cuda, val acc: 0.9369, tta: False, dtype: torch.bfloat16, model type: scripted model, took: 0.417161833s\n",
      "Device cuda, val acc: 0.9369, tta: False, dtype: torch.bfloat16, model type: traced model, took: 0.822116172s\n",
      "Device cuda, val acc: 0.9369, tta: False, dtype: torch.bfloat16, model type: frozen model, took: 1.645139727s\n",
      "Model type optimized for inference failed on torch.bfloat16 on cuda\n",
      "Device cuda, val acc: 0.9369, tta: False, dtype: torch.bfloat16, model type: compiled model, took: 4.584676277s\n",
      "\n",
      "Device cuda, val acc: 0.9367, tta: False, dtype: torch.float16, model type: raw model, took: 1.005549767s\n",
      "Device cuda, val acc: 0.9369, tta: False, dtype: torch.float16, model type: scripted model, took: 0.389272272s\n",
      "Device cuda, val acc: 0.9367, tta: False, dtype: torch.float16, model type: traced model, took: 0.8316144s\n",
      "Device cuda, val acc: 0.9368, tta: False, dtype: torch.float16, model type: frozen model, took: 1.031434852s\n",
      "Model type optimized for inference failed on torch.float16 on cuda\n",
      "Device cuda, val acc: 0.9369, tta: False, dtype: torch.float16, model type: compiled model, took: 4.276787482s\n",
      "\n",
      "Device cuda, val acc: 0.937, tta: False, dtype: torch.float32, model type: raw model, took: 1.967798217s\n",
      "Device cuda, val acc: 0.937, tta: False, dtype: torch.float32, model type: scripted model, took: 1.047288403s\n",
      "Device cuda, val acc: 0.937, tta: False, dtype: torch.float32, model type: traced model, took: 1.263171835s\n",
      "Device cuda, val acc: 0.937, tta: False, dtype: torch.float32, model type: frozen model, took: 0.82889755s\n",
      "Device cuda, val acc: 0.937, tta: False, dtype: torch.float32, model type: optimized for inference, took: 0.961604172s\n",
      "Device cuda, val acc: 0.9369, tta: False, dtype: torch.float32, model type: compiled model, took: 4.874611037s\n",
      "\n",
      "Device cuda, val acc: 0.9423, tta: True, dtype: torch.bfloat16, model type: raw model, took: 2.790701892s\n",
      "Device cuda, val acc: 0.9423, tta: True, dtype: torch.bfloat16, model type: scripted model, took: 1.369089757s\n",
      "Device cuda, val acc: 0.9423, tta: True, dtype: torch.bfloat16, model type: traced model, took: 1.46232627s\n",
      "Device cuda, val acc: 0.9424, tta: True, dtype: torch.bfloat16, model type: frozen model, took: 3.233142207s\n",
      "Model type optimized for inference failed on torch.bfloat16 on cuda\n",
      "Device cuda, val acc: 0.9422, tta: True, dtype: torch.bfloat16, model type: compiled model, took: 3.973672546s\n",
      "\n",
      "Device cuda, val acc: 0.9426, tta: True, dtype: torch.float16, model type: raw model, took: 2.343737406s\n",
      "Device cuda, val acc: 0.9425, tta: True, dtype: torch.float16, model type: scripted model, took: 1.983999274s\n",
      "Device cuda, val acc: 0.9426, tta: True, dtype: torch.float16, model type: traced model, took: 1.419569017s\n",
      "Device cuda, val acc: 0.9426, tta: True, dtype: torch.float16, model type: frozen model, took: 2.675022215s\n",
      "Model type optimized for inference failed on torch.float16 on cuda\n",
      "Device cuda, val acc: 0.9426, tta: True, dtype: torch.float16, model type: compiled model, took: 5.118906102s\n",
      "\n",
      "Device cuda, val acc: 0.9425, tta: True, dtype: torch.float32, model type: raw model, took: 3.308498991s\n",
      "Device cuda, val acc: 0.9425, tta: True, dtype: torch.float32, model type: scripted model, took: 2.468787608s\n",
      "Device cuda, val acc: 0.9425, tta: True, dtype: torch.float32, model type: traced model, took: 4.28679457s\n",
      "Device cuda, val acc: 0.9425, tta: True, dtype: torch.float32, model type: frozen model, took: 4.123212328s\n",
      "Device cuda, val acc: 0.9425, tta: True, dtype: torch.float32, model type: optimized for inference, took: 1.973731137s\n",
      "Device cuda, val acc: 0.9426, tta: True, dtype: torch.float32, model type: compiled model, took: 6.133027864s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predict(checkpoint_path='./', device=torch.device('cuda:1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95cd91b-ef72-4d95-8a44-3d6f50591e7f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
